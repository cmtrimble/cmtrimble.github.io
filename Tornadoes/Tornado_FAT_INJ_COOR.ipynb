{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Title: DSC350 Project  \n",
    "Author: Caleb Trimble  \n",
    "Date: 13 November 2024  \n",
    "Description: This program ingests and transforms multiple data sources from different data types to show the impact of tornadoes, along with the correlations between them and the damage they cause.  \n",
    "Codes in this program have been adapted from Python Data Analysis-Second Edition (Fandango A., 2017) and Hands-On Data Analysis with Pandas - Second Edition (Molin S., 2021)."
   ],
   "id": "1588360a028088c9"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-13T20:15:46.252912Z",
     "start_time": "2024-11-13T20:15:44.502589Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('1950-2019_all_tornadoes.csv')\n",
    "df.head()\n",
    "print(df.columns)\n",
    "# Transformation 1 - Creates DataFrame and drops columns that are irrelevant for the intended purpose.\n",
    "df_coldrop = df.drop(columns=['om', 'stf', 'stn', 'tz', 'ns', 'sn', 'sg', 'mo', 'dy', 'f1','f2','f3','f4','fc'])\n",
    "# Transformation 2 - Creates a column for datetime by combining 'date' and 'time' columns. Defines format for time, and drops the original columns.\n",
    "df_coldrop['datetime'] = pd.to_datetime(df_coldrop['date'] + ' ' + df_coldrop['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_coldrop.drop(columns=['date', 'time'], inplace=True)\n",
    "# Transformation 3 - Renames abbreviated columns for uniformity.\n",
    "df_coldrop.rename(columns={\n",
    "    'closs': 'Crop Damage',\n",
    "    'loss': 'Property Damage',\n",
    "    'yr' : 'Year',\n",
    "    'st' : 'state',\n",
    "    'inj' : 'injuries',\n",
    "    'fat' : 'fatalities',\n",
    "    'len' : 'length',\n",
    "    'wid' : 'width',\n",
    "    'mag' : 'F-Scale'\n",
    "}, inplace=True)\n",
    "# Transformation 4 - Combines lat and lon into a single column as tuples and drops the original columns\n",
    "df_coldrop.loc[:, 'start_coords'] = df_coldrop.apply(lambda row: (row['slat'], row['slon']), axis=1)\n",
    "df_coldrop.loc[:, 'end_coords'] = df_coldrop.apply(lambda row: (row['elat'], row['elon']), axis=1)\n",
    "df_coldrop.drop(columns=['slat', 'slon', 'elat', 'elon'], inplace=True)\n",
    "# Transformation 5 - Changes the order of columns to read more fluently.\n",
    "ordered = ['Year', 'state', 'start_coords', 'end_coords', 'datetime', 'length', 'width', 'F-Scale', 'injuries', 'fatalities', 'Crop Damage', 'Property Damage']\n",
    "df_coldrop = df_coldrop[ordered]\n",
    "df_coldrop.to_csv('tornado_flat.csv', index=False)\n",
    "print(\"Flat data has been saved to tornado_flat.csv\")\n",
    "print(df_coldrop)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['om', 'yr', 'mo', 'dy', 'date', 'time', 'tz', 'st', 'stf', 'stn', 'mag',\n",
      "       'inj', 'fat', 'loss', 'closs', 'slat', 'slon', 'elat', 'elon', 'len',\n",
      "       'wid', 'ns', 'sn', 'sg', 'f1', 'f2', 'f3', 'f4', 'fc'],\n",
      "      dtype='object')\n",
      "Flat data has been saved to tornado_flat.csv\n",
      "       Year state         start_coords           end_coords  \\\n",
      "0      1950    MO      (38.77, -90.22)      (38.83, -90.03)   \n",
      "1      1950    MO      (38.77, -90.22)      (38.82, -90.12)   \n",
      "2      1950    IL      (38.82, -90.12)      (38.83, -90.03)   \n",
      "3      1950    IL        (39.1, -89.3)      (39.12, -89.23)   \n",
      "4      1950    OH      (40.88, -84.58)           (0.0, 0.0)   \n",
      "...     ...   ...                  ...                  ...   \n",
      "66383  2019    MS  (33.1628, -89.4323)  (33.2339, -89.3298)   \n",
      "66384  2019    MS  (33.2598, -89.2778)  (33.2879, -89.2208)   \n",
      "66385  2019    MS   (33.472, -89.0315)   (33.4888, -88.991)   \n",
      "66386  2019    MS  (32.5268, -89.1628)  (32.5581, -89.1215)   \n",
      "66387  2019    AL  (34.7541, -87.0777)  (34.7946, -87.0041)   \n",
      "\n",
      "                 datetime  length  width  F-Scale  injuries  fatalities  \\\n",
      "0     1950-01-03 11:00:00    9.50    150        3         3           0   \n",
      "1     1950-01-03 11:00:00    6.20    150        3         3           0   \n",
      "2     1950-01-03 11:10:00    3.30    100        3         0           0   \n",
      "3     1950-01-03 11:55:00    3.60    130        3         3           0   \n",
      "4     1950-01-03 16:00:00    0.10     10        1         1           0   \n",
      "...                   ...     ...    ...      ...       ...         ...   \n",
      "66383 2019-12-29 16:03:00    7.70    900        1         0           0   \n",
      "66384 2019-12-29 16:13:00    3.82    200        1         0           0   \n",
      "66385 2019-12-29 16:32:00    2.61    200        0         0           0   \n",
      "66386 2019-12-29 17:13:00    3.23    125        1         0           0   \n",
      "66387 2019-12-29 18:50:00    5.07     50        0         0           0   \n",
      "\n",
      "       Crop Damage  Property Damage  \n",
      "0              0.0              6.0  \n",
      "1              0.0              6.0  \n",
      "2              0.0              5.0  \n",
      "3              0.0              5.0  \n",
      "4              0.0              4.0  \n",
      "...            ...              ...  \n",
      "66383          0.0          75000.0  \n",
      "66384          0.0          10000.0  \n",
      "66385          0.0           5000.0  \n",
      "66386          0.0         150000.0  \n",
      "66387          0.0              0.0  \n",
      "\n",
      "[66388 rows x 12 columns]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T20:12:13.643903Z",
     "start_time": "2024-11-13T20:12:08.789155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Base URL of the API\n",
    "url = 'https://services2.arcgis.com/FiaPA4ga0iQKduv3/arcgis/rest/services/Tornado_Tracks_1950_2017_1/FeatureServer/0/query'\n",
    "\n",
    "# Parameters to handle pagination and ensure proper query\n",
    "params = {\n",
    "    'where': 'yr >= 2009 AND yr <= 2019',  # Filter by year range\n",
    "    'outFields': '*',\n",
    "    'outSR': 4326,\n",
    "    'f': 'json',\n",
    "    'resultRecordCount': 1000,  # Limit for each request\n",
    "    'resultOffset': 0  # Starting point for records\n",
    "}\n",
    "\n",
    "# List to collect all data\n",
    "all_data = []\n",
    "\n",
    "# Loop to paginate through the API response\n",
    "while True:\n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        features = data.get('features', [])\n",
    "        \n",
    "        if not features:\n",
    "            break  # Break the loop if no more results\n",
    "        \n",
    "        attributes = [feature['attributes'] for feature in features]\n",
    "        all_data.extend(attributes)\n",
    "        \n",
    "        # Update the result offset for the next page\n",
    "        params['resultOffset'] += params['resultRecordCount']\n",
    "    else:\n",
    "        print(f'Error: {response.status_code}')\n",
    "        break\n",
    "\n",
    "# Convert the collected data to a DataFrame\n",
    "df_api = pd.DataFrame(all_data)\n",
    "print(df_api.columns)\n",
    "\n",
    "# Continue only if there are columns to drop\n",
    "if not df_api.empty:\n",
    "    # Adjust column names to match what's in the DataFrame\n",
    "    df_api.columns = map(str.lower, df_api.columns)  # Standardize to lower case for consistency\n",
    "\n",
    "    # Remove the first row of the HTML dataset\n",
    "    df_api.drop(index=0, inplace=True)\n",
    "\n",
    "    # Check which columns exist in the DataFrame\n",
    "    existing_columns = df_api.columns.tolist()\n",
    "    columns_to_drop = [\n",
    "        'objectid', 'om', 'mo', 'dy', 'tz', 'stf', 'month_calc', 'date_calc', 'shape__length'\n",
    "    ]\n",
    "    columns_to_drop = [col for col in columns_to_drop if col in existing_columns]\n",
    "    \n",
    "    # Drop irrelevant columns\n",
    "    df_api_coldrop = df_api.drop(columns=columns_to_drop, errors='ignore')\n",
    "    \n",
    "    # Ensure 'date' and 'time' columns are present before combining\n",
    "    if 'date' in df_api_coldrop.columns and 'time' in df_api_coldrop.columns:\n",
    "        # Transformation 2 - Creates a column for datetime by combining 'date' and 'time' columns\n",
    "        df_api_coldrop['datetime'] = pd.to_datetime(df_api_coldrop['date'] + ' ' + df_api_coldrop['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "        df_api_coldrop.drop(columns=['date', 'time'], inplace=True)\n",
    "    \n",
    "    # Transformation 3 - Renames abbreviated columns for uniformity\n",
    "    df_api_coldrop.rename(columns={\n",
    "        'closs': 'Crop Damage',\n",
    "        'loss': 'Property Damage',\n",
    "        'yr': 'Year',\n",
    "        'st': 'state',\n",
    "        'inj': 'injuries',\n",
    "        'fat': 'fatalities',\n",
    "        'len': 'length',\n",
    "        'wid': 'width',\n",
    "        'mag': 'F-Scale'\n",
    "    }, inplace=True, errors='ignore')\n",
    "    \n",
    "    # Transformation 4 - Combines lat and lon into a single column as tuples and drops the original columns\n",
    "    df_api_coldrop.loc[:, 'start_coords'] = df_api_coldrop.apply(lambda row: (row['slat'], row['slon']), axis=1)\n",
    "    df_api_coldrop.loc[:, 'end_coords'] = df_api_coldrop.apply(lambda row: (row['elat'], row['elon']), axis=1)\n",
    "    df_api_coldrop.drop(columns=['slat', 'slon', 'elat', 'elon'], inplace=True, errors='ignore')\n",
    "    \n",
    "    # Transformation 5 - Changes the order of columns to read more fluently\n",
    "    ordered = ['Year', 'state', 'start_coords', 'end_coords', 'datetime', 'length', 'width', 'F-Scale', 'injuries', 'fatalities', 'Crop Damage', 'Property Damage']\n",
    "    df_api_coldrop = df_api_coldrop[ordered]\n",
    "    \n",
    "    # Save the transformed DataFrame to a CSV file\n",
    "    df_api_coldrop.to_csv('tornado_API.csv', index=False)\n",
    "    print(\"API data has been saved to tornado_API.csv\")\n",
    "    \n",
    "    print(df_api_coldrop.head())\n",
    "else:\n",
    "    print(\"No data retrieved from API.\")\n"
   ],
   "id": "55f86bd58accfb9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['OBJECTID', 'om', 'yr', 'mo', 'dy', 'date', 'time', 'tz', 'st', 'stf',\n",
      "       'stn', 'mag', 'inj', 'fat', 'loss', 'closs', 'slat', 'slon', 'elat',\n",
      "       'elon', 'len', 'wid', 'fc', 'Month_Calc', 'Date_Calc', 'Shape__Length'],\n",
      "      dtype='object')\n",
      "API data has been saved to tornado_API.csv\n",
      "   Year state         start_coords           end_coords            datetime  \\\n",
      "1  2009    GA       (32.0, -84.25)      (31.99, -84.22) 2009-10-15 17:00:00   \n",
      "2  2009    LA  (29.7605, -93.0051)  (29.7605, -93.0051) 2009-10-22 09:32:00   \n",
      "3  2009    LA  (31.9969, -93.4688)  (32.0037, -93.4668) 2009-10-22 10:20:00   \n",
      "4  2009    LA      (30.05, -92.75)  (30.1647, -92.7268) 2009-10-22 11:00:00   \n",
      "5  2009    LA   (30.2132, -92.716)  (30.4479, -92.7097) 2009-10-22 11:22:00   \n",
      "\n",
      "   length  width  F-Scale  injuries  fatalities  Crop Damage  Property Damage  \n",
      "1    2.11    100        1         0           0          0.0             0.13  \n",
      "2    0.10     10        0         0           0          0.0             0.00  \n",
      "3    0.48     75        0         0           0          0.0             0.00  \n",
      "4    8.05     50        0         0           0          0.0             0.02  \n",
      "5   16.22    800        1         1           0          0.0             0.25  \n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T16:02:01.280685Z",
     "start_time": "2024-11-13T16:01:58.553804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Fetch and Parse HTML\n",
    "url = 'https://data.usatoday.com/tornado-archive/'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Using table_id, searches for the called on table\n",
    "    table = soup.find('table', {'id': 'tornadoSummary'})\n",
    "    \n",
    "    if table:\n",
    "        # Extracts Table Data\n",
    "        rows = table.find_all('tr')\n",
    "        \n",
    "        data = []\n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            cols = [ele.text.strip() for ele in cols if ele.text.strip()]\n",
    "            if cols:\n",
    "                data.append(cols)\n",
    "        \n",
    "        # Adjusts columns based on actual data structure\n",
    "        columns = ['Year', 'Number of Tornadoes', 'Direct Injury', 'Indirect Injury', 'Direct Fatality', 'Indirect Fatality', 'Property Damage', 'Crop Damage']\n",
    "\n",
    "        # Transformation 1 - Creates DataFrame from table data.\n",
    "        html_df = pd.DataFrame(data, columns=columns)\n",
    "        html_df.drop(index=0, inplace=True)\n",
    "        # Transformation 2 - Merges direct and indirect Columns\n",
    "        html_df['injuries'] = pd.to_numeric(html_df['Direct Injury'], errors='coerce') + pd.to_numeric(html_df['Indirect Injury'], errors='coerce')\n",
    "        html_df['fatalities'] = pd.to_numeric(html_df['Direct Fatality'], errors='coerce') + pd.to_numeric(html_df['Indirect Fatality'], errors='coerce')\n",
    "\n",
    "        # Transformation 3 - drops unnecessary columns.  \n",
    "        html_df.drop(columns=['Direct Injury', 'Indirect Injury', 'Direct Fatality', 'Indirect Fatality'], inplace=True)\n",
    "\n",
    "        # Transformation 4 - Cleans and converts Damage Columns\n",
    "        html_df['Property Damage'] = html_df['Property Damage'].str.replace(',', '').str.replace('$', '') \n",
    "        html_df['Crop Damage'] = html_df['Crop Damage'].str.replace(',', '').str.replace('$', '')\n",
    "\n",
    "        # Transformation 5 - Converts datatypes to Numeric\n",
    "        html_df['Property Damage'] = pd.to_numeric(html_df['Property Damage'], errors='coerce')\n",
    "        html_df['Crop Damage'] = pd.to_numeric(html_df['Crop Damage'], errors='coerce')\n",
    "        # Save the transformed DataFrame to a CSV file\n",
    "        html_df.to_csv('tornado_html.csv', index=False)\n",
    "        print(\"HTML data has been saved to tornado_html.csv\")\n",
    "\n",
    "        print(html_df.head())\n",
    "    else:\n",
    "        print(\"Failed to locate the table with id 'tornadoSummary' on the webpage.\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n"
   ],
   "id": "617fdc99457daa41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML data has been saved to html_data_transformed.csv\n",
      "   Year Number of Tornadoes  Property Damage  Crop Damage  injuries  \\\n",
      "1  2024               1,664        935078800   13326200.0     544.0   \n",
      "2  2023               1,523       1371376500    7300800.0     955.0   \n",
      "3  2022               1,384        698683090    5550500.0     318.0   \n",
      "4  2021               1,545        232623000    2238400.0     881.0   \n",
      "5  2020               1,251       2504035500   27261500.0     741.0   \n",
      "\n",
      "   fatalities  \n",
      "1          45  \n",
      "2          91  \n",
      "3          25  \n",
      "4         107  \n",
      "5          77  \n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T16:06:04.040194Z",
     "start_time": "2024-11-13T16:03:25.530205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Load the transformed flat file CSV into SQLite3\n",
    "df_flatfile = pd.read_csv('tornado_flat.csv')\n",
    "\n",
    "# Ensure proper connection handling\n",
    "with sqlite3.connect('DSC350_Project.db', timeout=10) as conn:\n",
    "    df_flatfile.to_sql('FlatFile_Table', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Load the API CSV into SQLite3\n",
    "df_api = pd.read_csv('tornado_API.csv')\n",
    "\n",
    "# Ensure proper connection handling\n",
    "with sqlite3.connect('DSC350_Project.db') as conn:\n",
    "    df_api.to_sql('API_Table', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Load the HTML CSV into SQLite3\n",
    "df_html = pd.read_csv('tornado_html.csv')\n",
    "\n",
    "# Ensure proper connection handling\n",
    "with sqlite3.connect('DSC350_Project.db') as conn:\n",
    "    df_html.to_sql('HTML_Table', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Query to merge the datasets\n",
    "query = '''\n",
    "    SELECT *\n",
    "    FROM FlatFile_Table AS ff\n",
    "    JOIN API_Table AS api ON ff.year = api.year\n",
    "    JOIN HTML_Table AS html ON ff.year = html.Year\n",
    "'''\n",
    "\n",
    "# Process the query in smaller chunks\n",
    "chunks = []\n",
    "with sqlite3.connect('DSC350_Project.db') as conn:\n",
    "    for chunk in pd.read_sql_query(query, conn, chunksize=5000):  # Adjust chunk size as needed\n",
    "        chunks.append(chunk)\n",
    "\n",
    "merged_df = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# Display the merged dataset\n",
    "print(merged_df.head())\n"
   ],
   "id": "8be64a314140d1cb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_15192\\2895886704.py:39: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat(chunks, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year state       start_coords         end_coords             datetime  \\\n",
      "0  2009    MS  (31.875, -89.331)  (31.888, -89.327)  2009-01-03 13:13:00   \n",
      "1  2009    MS  (31.875, -89.331)  (31.888, -89.327)  2009-01-03 13:13:00   \n",
      "2  2009    MS  (31.875, -89.331)  (31.888, -89.327)  2009-01-03 13:13:00   \n",
      "3  2009    MS  (31.875, -89.331)  (31.888, -89.327)  2009-01-03 13:13:00   \n",
      "4  2009    MS  (31.875, -89.331)  (31.888, -89.327)  2009-01-03 13:13:00   \n",
      "\n",
      "   length  width F-Scale  injuries  fatalities  ...  width F-Scale  injuries  \\\n",
      "0    0.93    175       1         0           0  ...     50       0         0   \n",
      "1    0.93    175       1         0           0  ...   1000       1         0   \n",
      "2    0.93    175       1         0           0  ...    200       1         0   \n",
      "3    0.93    175       1         0           0  ...     30       0         0   \n",
      "4    0.93    175       1         0           0  ...     50       0         0   \n",
      "\n",
      "   fatalities  Year  Number of Tornadoes  Property Damage  Crop Damage  \\\n",
      "0           0  2009                1,273        565864100   18477500.0   \n",
      "1           0  2009                1,273        565864100   18477500.0   \n",
      "2           0  2009                1,273        565864100   18477500.0   \n",
      "3           0  2009                1,273        565864100   18477500.0   \n",
      "4           0  2009                1,273        565864100   18477500.0   \n",
      "\n",
      "   injuries  fatalities  \n",
      "0     407.0          25  \n",
      "1     407.0          25  \n",
      "2     407.0          25  \n",
      "3     407.0          25  \n",
      "4     407.0          25  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note of datasource change:  \n",
    "I was unhappy with the data provided on the opendatasoft API, so I chose to change the API to FEMA to ensure data integrity. "
   ],
   "id": "65b6f1583c57d9de"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
